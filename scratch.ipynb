{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/51eb0667-f71d-4fe0-a83e-beaff24c04fb/anaconda/envs/vaanienv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "DATASET_PATH = \"sudoku_sft_data.json\"\n",
    "OUTPUT_DIR = \"sft_output\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"instruction\"] + examples[\"input\"]\n",
    "    targets = examples[\"output\"]\n",
    "    text = tokenizer(f\"<instruction>, {inputs}, <output>, {targets}{tokenizer.eos_token}\", return_tensors=\"pt\", padding=\"longest\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"] = data[\"train\"].take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Solve this Sudoku puzzle:',\n",
       " 'input': '0 1 9 4 0 3 0 5 7 2 5 3 7 9 1 0 4 8 4 0 8 5 6 2 0 1 9 0 9 1 2 0 6 0 7 5 5 2 0 1 7 8 4 9 3 8 4 0 3 5 9 1 0 2 7 3 2 6 1 5 9 8 4 9 6 5 0 3 0 0 2 1 1 8 4 9 2 7 5 3 0',\n",
       " 'output': '<thonk> I see a sudoku problem. Most of its cells are filled. So it should be easy to finish it.\\nIn row 2 the only missing element is 6 so row 2 column 7 must be 6.\\nIn column 2 the only missing element is 7 so row 3 column 2 must be 7.\\nIn row 3 the only missing element is 3 so row 3 column 7 must be 3.\\nIn column 4 the only missing element is 8 so row 8 column 4 must be 8.\\nIn row 5 the only missing element is 6 so row 5 column 3 must be 6.\\nIn column 3 the only missing element is 7 so row 6 column 3 must be 7.\\nIn row 6 the only missing element is 6 so row 6 column 8 must be 6.\\nIn column 6 the only missing element is 4 so row 8 column 6 must be 4.\\nIn row 8 the only missing element is 7 so row 8 column 7 must be 7.\\nIn column 9 the only missing element is 6 so row 9 column 9 must be 6.\\nIn block (1, 1) the only missing element is 6 so row 1 column 1 must be 6.\\nIn column 1 the only missing element is 3 so row 4 column 1 must be 3.\\nIn block (1, 2) the only missing element is 8 so row 1 column 5 must be 8.\\nIn column 5 the only missing element is 4 so row 4 column 5 must be 4.\\nIn row 1 the only missing element is 2 so row 1 column 7 must be 2.\\nIn column 7 the only missing element is 8 so row 4 column 7 must be 8.\\nI think this completes the sudoku. Let me check:\\n6 1 9 4 8 3 2 5 7\\n2 5 3 7 9 1 6 4 8\\n4 7 8 5 6 2 3 1 9\\n3 9 1 2 4 6 8 7 5\\n5 2 6 1 7 8 4 9 3\\n8 4 7 3 5 9 1 6 2\\n7 3 2 6 1 5 9 8 4\\n9 6 5 8 3 4 7 2 1\\n1 8 4 9 2 7 5 3 6\\nLets see if it satisfies the sudoku rules. </thonk>\\n<ans>\\n619483257253791648478562319391246875526178493847359162732615984965834721184927536\\n</ans>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 135.80 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    27,  54974,   8066,  63284,    419,  94254,  24626,     25,     15,\n",
       "            220,     16,    220,     24,    220,     19,    220,     15,    220,\n",
       "             18,    220,     15,    220,     20,    220,     22,    220,     17,\n",
       "            220,     20,    220,     18,    220,     22,    220,     24,    220,\n",
       "             16,    220,     15,    220,     19,    220,     23,    220,     19,\n",
       "            220,     15,    220,     23,    220,     20,    220,     21,    220,\n",
       "             17,    220,     15,    220,     16,    220,     24,    220,     15,\n",
       "            220,     24,    220,     16,    220,     17,    220,     15,    220,\n",
       "             21,    220,     15,    220,     22,    220,     20,    220,     20,\n",
       "            220,     17,    220,     15,    220,     16,    220,     22,    220,\n",
       "             23,    220,     19,    220,     24,    220,     18,    220,     23,\n",
       "            220,     19,    220,     15,    220,     18,    220,     20,    220,\n",
       "             24,    220,     16,    220,     15,    220,     17,    220,     22,\n",
       "            220,     18,    220,     17,    220,     21,    220,     16,    220,\n",
       "             20,    220,     24,    220,     23,    220,     19,    220,     24,\n",
       "            220,     21,    220,     20,    220,     15,    220,     18,    220,\n",
       "             15,    220,     15,    220,     17,    220,     16,    220,     16,\n",
       "            220,     23,    220,     19,    220,     24,    220,     17,    220,\n",
       "             22,    220,     20,    220,     18,    220,     15,     11,    366,\n",
       "           3006,   8066,    366,   4587,     74,     29,    358,   1490,    264,\n",
       "          90809,   3491,     13,   7496,    315,   1181,   7761,    525,  10199,\n",
       "             13,   2055,    432,   1265,    387,   4135,    311,   6248,    432,\n",
       "            624,    641,   2802,    220,     17,    279,   1172,   7402,   2392,\n",
       "            374,    220,     21,    773,   2802,    220,     17,   3250,    220,\n",
       "             22,   1969,    387,    220,     21,    624,    641,   3250,    220,\n",
       "             17,    279,   1172,   7402,   2392,    374,    220,     22,    773,\n",
       "           2802,    220,     18,   3250,    220,     17,   1969,    387,    220,\n",
       "             22,    624,    641,   2802,    220,     18,    279,   1172,   7402,\n",
       "           2392,    374,    220,     18,    773,   2802,    220,     18,   3250,\n",
       "            220,     22,   1969,    387,    220,     18,    624,    641,   3250,\n",
       "            220,     19,    279,   1172,   7402,   2392,    374,    220,     23,\n",
       "            773,   2802,    220,     23,   3250,    220,     19,   1969,    387,\n",
       "            220,     23,    624,    641,   2802,    220,     20,    279,   1172,\n",
       "           7402,   2392,    374,    220,     21,    773,   2802,    220,     20,\n",
       "           3250,    220,     18,   1969,    387,    220,     21,    624,    641,\n",
       "           3250,    220,     18,    279,   1172,   7402,   2392,    374,    220,\n",
       "             22,    773,   2802,    220,     21,   3250,    220,     18,   1969,\n",
       "            387,    220,     22,    624,    641,   2802,    220,     21,    279,\n",
       "           1172,   7402,   2392,    374,    220,     21,    773,   2802,    220,\n",
       "             21,   3250,    220,     23,   1969,    387,    220,     21,    624,\n",
       "            641,   3250,    220,     21,    279,   1172,   7402,   2392,    374,\n",
       "            220,     19,    773,   2802,    220,     23,   3250,    220,     21,\n",
       "           1969,    387,    220,     19,    624,    641,   2802,    220,     23,\n",
       "            279,   1172,   7402,   2392,    374,    220,     22,    773,   2802,\n",
       "            220,     23,   3250,    220,     22,   1969,    387,    220,     22,\n",
       "            624,    641,   3250,    220,     24,    279,   1172,   7402,   2392,\n",
       "            374,    220,     21,    773,   2802,    220,     24,   3250,    220,\n",
       "             24,   1969,    387,    220,     21,    624,    641,   2504,    320,\n",
       "             16,     11,    220,     16,      8,    279,   1172,   7402,   2392,\n",
       "            374,    220,     21,    773,   2802,    220,     16,   3250,    220,\n",
       "             16,   1969,    387,    220,     21,    624,    641,   3250,    220,\n",
       "             16,    279,   1172,   7402,   2392,    374,    220,     18,    773,\n",
       "           2802,    220,     19,   3250,    220,     16,   1969,    387,    220,\n",
       "             18,    624,    641,   2504,    320,     16,     11,    220,     17,\n",
       "              8,    279,   1172,   7402,   2392,    374,    220,     23,    773,\n",
       "           2802,    220,     16,   3250,    220,     20,   1969,    387,    220,\n",
       "             23,    624,    641,   3250,    220,     20,    279,   1172,   7402,\n",
       "           2392,    374,    220,     19,    773,   2802,    220,     19,   3250,\n",
       "            220,     20,   1969,    387,    220,     19,    624,    641,   2802,\n",
       "            220,     16,    279,   1172,   7402,   2392,    374,    220,     17,\n",
       "            773,   2802,    220,     16,   3250,    220,     22,   1969,    387,\n",
       "            220,     17,    624,    641,   3250,    220,     22,    279,   1172,\n",
       "           7402,   2392,    374,    220,     23,    773,   2802,    220,     19,\n",
       "           3250,    220,     22,   1969,    387,    220,     23,    624,     40,\n",
       "           1744,    419,  44595,    279,  90809,     13,   6771,    752,   1779,\n",
       "            510,     21,    220,     16,    220,     24,    220,     19,    220,\n",
       "             23,    220,     18,    220,     17,    220,     20,    220,     22,\n",
       "            198,     17,    220,     20,    220,     18,    220,     22,    220,\n",
       "             24,    220,     16,    220,     21,    220,     19,    220,     23,\n",
       "            198,     19,    220,     22,    220,     23,    220,     20,    220,\n",
       "             21,    220,     17,    220,     18,    220,     16,    220,     24,\n",
       "            198,     18,    220,     24,    220,     16,    220,     17,    220,\n",
       "             19,    220,     21,    220,     23,    220,     22,    220,     20,\n",
       "            198,     20,    220,     17,    220,     21,    220,     16,    220,\n",
       "             22,    220,     23,    220,     19,    220,     24,    220,     18,\n",
       "            198,     23,    220,     19,    220,     22,    220,     18,    220,\n",
       "             20,    220,     24,    220,     16,    220,     21,    220,     17,\n",
       "            198,     22,    220,     18,    220,     17,    220,     21,    220,\n",
       "             16,    220,     20,    220,     24,    220,     23,    220,     19,\n",
       "            198,     24,    220,     21,    220,     20,    220,     23,    220,\n",
       "             18,    220,     19,    220,     22,    220,     17,    220,     16,\n",
       "            198,     16,    220,     23,    220,     19,    220,     24,    220,\n",
       "             17,    220,     22,    220,     20,    220,     18,    220,     21,\n",
       "            198,  93313,   1490,    421,    432,  67901,    279,  90809,   5601,\n",
       "             13,    690,   4587,     74,    397,     27,    596,    397,     21,\n",
       "             16,     24,     19,     23,     18,     17,     20,     22,     17,\n",
       "             20,     18,     22,     24,     16,     21,     19,     23,     19,\n",
       "             22,     23,     20,     21,     17,     18,     16,     24,     18,\n",
       "             24,     16,     17,     19,     21,     23,     22,     20,     20,\n",
       "             17,     21,     16,     22,     23,     19,     24,     18,     23,\n",
       "             19,     22,     18,     20,     24,     16,     21,     17,     22,\n",
       "             18,     17,     21,     16,     20,     24,     23,     19,     24,\n",
       "             21,     20,     23,     18,     19,     22,     17,     16,     16,\n",
       "             23,     19,     24,     17,     22,     20,     18,     21,    198,\n",
       "            522,    596,     29, 151645]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(preprocess_function, remove_columns=[\"instruction\", \"input\", \"output\"]).with_format(\"torch\")\n",
    "tokenized_datasets[\"train\"][0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[44047,    29]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<s>\", return_tensors=\"pt\", padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 850])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9330, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenized_datasets[\"train\"][0][\"input_ids\"].to(\"cuda\")\n",
    "print(input_ids.shape)\n",
    "model(input_ids, labels=input_ids).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190021/744939746.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"sft_output/epoch=0-step=11250.ckpt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"sft_output/epoch=0-step=11250.ckpt\")\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {}\n",
    "for key in checkpoint[\"state_dict\"].keys():\n",
    "    # remove the prefix \"model.\"\n",
    "    state_dict[key.replace(\"model.\", \"\", 1)] = checkpoint[\"state_dict\"][key]\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<instruction>, Solve this Sudoku puzzle:\\nSolve this Sudoku puzzle:0 1 9 4 0 3 0 5 7 2 5 3 7 9 1 0 4 8 4 0 8 5 6 2 0 1 9 0 9 1 2 0 6 0 7 5 5 2 0 1 7 8 4 9 3 8 4 0 3 5 9 1 0 2 7 3 2 6 1 5 9 8 4 9 6 5 0 3 0 0 2 1 1 8 4 9 2 7 5 3 0, <output>, ',\n",
       " {'input_ids': tensor([[   27, 54974,  8066, 63284,   419, 94254, 24626,   510,    50,  3948,\n",
       "            419, 94254, 24626,    25,    15,   220,    16,   220,    24,   220,\n",
       "             19,   220,    15,   220,    18,   220,    15,   220,    20,   220,\n",
       "             22,   220,    17,   220,    20,   220,    18,   220,    22,   220,\n",
       "             24,   220,    16,   220,    15,   220,    19,   220,    23,   220,\n",
       "             19,   220,    15,   220,    23,   220,    20,   220,    21,   220,\n",
       "             17,   220,    15,   220,    16,   220,    24,   220,    15,   220,\n",
       "             24,   220,    16,   220,    17,   220,    15,   220,    21,   220,\n",
       "             15,   220,    22,   220,    20,   220,    20,   220,    17,   220,\n",
       "             15,   220,    16,   220,    22,   220,    23,   220,    19,   220,\n",
       "             24,   220,    18,   220,    23,   220,    19,   220,    15,   220,\n",
       "             18,   220,    20,   220,    24,   220,    16,   220,    15,   220,\n",
       "             17,   220,    22,   220,    18,   220,    17,   220,    21,   220,\n",
       "             16,   220,    20,   220,    24,   220,    23,   220,    19,   220,\n",
       "             24,   220,    21,   220,    20,   220,    15,   220,    18,   220,\n",
       "             15,   220,    15,   220,    17,   220,    16,   220,    16,   220,\n",
       "             23,   220,    19,   220,    24,   220,    17,   220,    22,   220,\n",
       "             20,   220,    18,   220,    15,    11,   366,  3006,  8066,   220]],\n",
       "        device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"<instruction>, Solve this Sudoku puzzle:\\n{data['train'][0]['instruction'] + data['train'][0]['input']}, <output>, \"\n",
    "y = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(\"cuda\")\n",
    "prompt, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudoku import Sudoku\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puzzle has exactly one solution\n",
      "+-------+-------+-------+\n",
      "| 4 1 9 |   3 8 | 7 6 2 |\n",
      "| 5 6 7 | 1 2 4 | 9 8 3 |\n",
      "| 2 3 8 | 7 9 6 | 4 5 1 |\n",
      "+-------+-------+-------+\n",
      "| 8 5 3 | 2 4 9 | 1 7 6 |\n",
      "| 9 4 1 |     7 | 2 3 8 |\n",
      "| 6 7 2 | 3 8 1 | 5 4 9 |\n",
      "+-------+-------+-------+\n",
      "|   8 4 | 9 7 3 | 6 2 5 |\n",
      "| 3 9   | 4 6 2 | 8 1 7 |\n",
      "| 7 2 6 | 8 1 5 |       |\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<instruction>, Solve this Sudoku puzzle:\\n419038762567124983238796451853249176941007238672381549084973625390462817726815000, <output>,',\n",
       " {'input_ids': tensor([[   27, 54974,  8066, 63284,   419, 94254, 24626,   510,    19,    16,\n",
       "             24,    15,    18,    23,    22,    21,    17,    20,    21,    22,\n",
       "             16,    17,    19,    24,    23,    18,    17,    18,    23,    22,\n",
       "             24,    21,    19,    20,    16,    23,    20,    18,    17,    19,\n",
       "             24,    16,    22,    21,    24,    19,    16,    15,    15,    22,\n",
       "             17,    18,    23,    21,    22,    17,    18,    23,    16,    20,\n",
       "             19,    24,    15,    23,    19,    24,    22,    18,    21,    17,\n",
       "             20,    18,    24,    15,    19,    21,    17,    23,    16,    22,\n",
       "             22,    17,    21,    23,    16,    20,    15,    15,    15,    11,\n",
       "            366,  3006,  8066]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0')})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sud = Sudoku().difficulty(0.1)\n",
    "sud.show()\n",
    "problem = \"\".join([str(cell or 0) for row in sud.board for cell in row])\n",
    "prompt = f\"<instruction>, Solve this Sudoku puzzle:\\n{problem}, <output>,\"\n",
    "y = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(\"cuda\")\n",
    "prompt, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<instruction>, Solve this Sudoku puzzle:\\n419038762567124983238796451853249176941007238672381549084973625390462817726815000, <output>,<thonk> I see a sudoku problem. Most of its cells are filled. So it should be easy to finish it.\\nIn column 1 the only missing element is 5 so row 9 column 1 must be 5.\\nIn column 5 the only missing element is 4 so row 9 column 5 must be 4.\\nIn column 7 the only missing element is 6 so row 9 column 7 must be 6.\\nIn block (1,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.generate(**y, max_new_tokens=100)\n",
    "tokenizer.decode(x[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaanienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
